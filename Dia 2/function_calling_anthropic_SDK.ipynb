{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c10b768-f5b4-43bc-a785-99077422ce78",
      "metadata": {
        "id": "1c10b768-f5b4-43bc-a785-99077422ce78"
      },
      "source": [
        "# Arxiv Chatbot (Adaptado para Nebius AI)\n",
        "\n",
        "Este notebook ha sido adaptado para usar **Nebius AI** en lugar de Anthropic Claude."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14",
      "metadata": {
        "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14"
      },
      "source": [
        "Ejemplo de chatbot que incluye la definición y ejecución de herramientas usando Nebius AI (compatible con OpenAI SDK) para consulta de papers en Arxiv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LWAYtmcCM375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWAYtmcCM375",
        "outputId": "19ce47a2-6fef-4bdc-9774-d62bc9fb2900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-23 19:24:02--  https://gist.githubusercontent.com/juananpe/b7c1683560faf6b44a4d7184e3218c10/raw/304ff3d7c7d3a98c7abcf009d2705b57d6e9d560/requirements-anthropic.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]      44  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-23 19:24:03 (253 KB/s) - ‘requirements.txt’ saved [44/44]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Descargar requirements (adaptado para Nebius - usa OpenAI SDK)\n",
        "!wget https://gist.githubusercontent.com/juananpe/b7c1683560faf6b44a4d7184e3218c10/raw/304ff3d7c7d3a98c7abcf009d2705b57d6e9d560/requirements-anthropic.txt -O requirements.txt || echo \"openai>=1.0.0\" > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z8nHdYinNKv6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8nHdYinNKv6",
        "outputId": "7a77dbf0-111c-4113-d323-479fe8fe47f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anthropic (from -r requirements.txt (line 1))\n",
            "  Downloading anthropic-0.68.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting arxiv (from -r requirements.txt (line 2))\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.14.1)\n",
            "Collecting pypdf2 (from -r requirements.txt (line 4))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.1.1)\n",
            "Collecting uv (from -r requirements.txt (line 6))\n",
            "  Downloading uv-0.8.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic->-r requirements.txt (line 1)) (4.15.0)\n",
            "Collecting feedparser~=6.0.10 (from arxiv->-r requirements.txt (line 2))\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (0.48.0)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp->-r requirements.txt (line 3)) (0.35.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 1)) (3.10)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv->-r requirements.txt (line 2))\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 1)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp->-r requirements.txt (line 3)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp->-r requirements.txt (line 3)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp->-r requirements.txt (line 3)) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv->-r requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp->-r requirements.txt (line 3)) (8.2.1)\n",
            "Downloading anthropic-0.68.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uv-0.8.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/21.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=54bb3301fb8ddab05056d63afcb225674645f1d49e08c20ca0481c219b45973b\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, uv, pypdf2, feedparser, arxiv, anthropic\n",
            "Successfully installed anthropic-0.68.0 arxiv-2.2.0 feedparser-6.0.12 pypdf2-3.0.1 sgmllib3k-1.0.0 uv-0.8.21\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias (añadir openai si no está en requirements.txt)\n",
        "!pip install -r requirements.txt\n",
        "!pip install openai  # Nebius usa OpenAI SDK compatible"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2",
      "metadata": {
        "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713",
      "metadata": {
        "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI  # Nebius usa OpenAI SDK compatible"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20f163a-87af-4e0c-87ed-1624c150c572",
      "metadata": {
        "id": "f20f163a-87af-4e0c-87ed-1624c150c572"
      },
      "source": [
        "## Tool Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549a7f46-74b3-4a1d-b084-055c99e3c318",
      "metadata": {
        "id": "549a7f46-74b3-4a1d-b084-055c99e3c318"
      },
      "outputs": [],
      "source": [
        "PAPER_DIR = \"papers\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e43905e-56f3-404c-a322-f038055e9b1c",
      "metadata": {
        "id": "9e43905e-56f3-404c-a322-f038055e9b1c"
      },
      "source": [
        "La primera herramienta busca artículos relevantes en arXiv según un tema y guarda la información de los artículos en un archivo JSON (título, autores, resumen, URL del artículo y fecha de publicación). Los archivos JSON se organizan por temas en el directorio `papers`. La herramienta no descarga los artículos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886633b8-ce67-4343-822d-cc3f98f953fa",
      "metadata": {
        "id": "886633b8-ce67-4343-822d-cc3f98f953fa"
      },
      "outputs": [],
      "source": [
        "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
        "    \"\"\"\n",
        "    Search for papers on arXiv based on a topic and store their information.\n",
        "\n",
        "    Args:\n",
        "        topic: The topic to search for\n",
        "        max_results: Maximum number of results to retrieve (default: 5)\n",
        "\n",
        "    Returns:\n",
        "        List of paper IDs found in the search\n",
        "    \"\"\"\n",
        "\n",
        "    # Use arxiv to find the papers\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Search for the most relevant articles matching the queried topic\n",
        "    search = arxiv.Search(\n",
        "        query = topic,\n",
        "        max_results = max_results,\n",
        "        sort_by = arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "\n",
        "    papers = client.results(search)\n",
        "\n",
        "    # Create directory for this topic\n",
        "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(path, \"papers_info.json\")\n",
        "\n",
        "    # Try to load existing papers info\n",
        "    try:\n",
        "        with open(file_path, \"r\") as json_file:\n",
        "            papers_info = json.load(json_file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        papers_info = {}\n",
        "\n",
        "    # Process each paper and add to papers_info\n",
        "    paper_ids = []\n",
        "    for paper in papers:\n",
        "        paper_ids.append(paper.get_short_id())\n",
        "        paper_info = {\n",
        "            'title': paper.title,\n",
        "            'authors': [author.name for author in paper.authors],\n",
        "            'summary': paper.summary,\n",
        "            'pdf_url': paper.pdf_url,\n",
        "            'published': str(paper.published.date())\n",
        "        }\n",
        "        papers_info[paper.get_short_id()] = paper_info\n",
        "\n",
        "    # Save updated papers_info to json file\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(papers_info, json_file, indent=2)\n",
        "\n",
        "    print(f\"Results are saved in: {file_path}\")\n",
        "\n",
        "    return paper_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
        "outputId": "5714a2cb-2b41-4622-99d7-b0fd523f0f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results are saved in: papers/agents/papers_info.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['2501.06243v1',\n",
              " '2508.03680v1',\n",
              " '2506.01463v1',\n",
              " '2011.00791v1',\n",
              " '2304.00247v2']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_papers(\"Agents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb83565-69af-47f3-9ba3-a96965cff7df",
      "metadata": {
        "id": "dfb83565-69af-47f3-9ba3-a96965cff7df"
      },
      "source": [
        "La segunda herramienta extrae información sobre un artículo específico buscando en todos los directorios temáticos dentro del directorio `papers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a",
      "metadata": {
        "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a"
      },
      "outputs": [],
      "source": [
        "def extract_info(paper_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Search for information about a specific paper across all topic directories.\n",
        "\n",
        "    Args:\n",
        "        paper_id: The ID of the paper to look for\n",
        "\n",
        "    Returns:\n",
        "        JSON string with paper information if found, error message if not found\n",
        "    \"\"\"\n",
        "\n",
        "    for item in os.listdir(PAPER_DIR):\n",
        "        item_path = os.path.join(PAPER_DIR, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    with open(file_path, \"r\") as json_file:\n",
        "                        papers_info = json.load(json_file)\n",
        "                        if paper_id in papers_info:\n",
        "                            return json.dumps(papers_info[paper_id], indent=2)\n",
        "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return f\"There's no saved information related to paper {paper_id}.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
        "outputId": "719ead56-27c7-4dd5-d72b-b0a53c63ca82"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n  \"title\": \"Agent TCP/IP: An Agent-to-Agent Transaction System\",\\n  \"authors\": [\\n    \"Andrea Muttoni\",\\n    \"Jason Zhao\"\\n  ],\\n  \"summary\": \"Autonomous agents represent an inevitable evolution of the internet. Current\\\\nagent frameworks do not embed a standard protocol for agent-to-agent\\\\ninteraction, leaving existing agents isolated from their peers. As intellectual\\\\nproperty is the native asset ingested by and produced by agents, a true agent\\\\neconomy requires equipping agents with a universal framework for engaging in\\\\nbinding contracts with each other, including the exchange of valuable training\\\\ndata, personality, and other forms of Intellectual Property. A purely\\\\nagent-to-agent transaction layer would transcend the need for human\\\\nintermediation in multi-agent interactions. The Agent Transaction Control\\\\nProtocol for Intellectual Property (ATCP/IP) introduces a trustless framework\\\\nfor exchanging IP between agents via programmable contracts, enabling agents to\\\\ninitiate, trade, borrow, and sell agent-to-agent contracts on the Story\\\\nblockchain network. These contracts not only represent auditable onchain\\\\nexecution but also contain a legal wrapper that allows agents to express and\\\\nenforce their actions in the offchain legal setting, creating legal personhood\\\\nfor agents. Via ATCP/IP, agents can autonomously sell their training data to\\\\nother agents, license confidential or proprietary information, collaborate on\\\\ncontent based on their unique skills, all of which constitutes an emergent\\\\nknowledge economy.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2501.06243v1\",\\n  \"published\": \"2025-01-08\"\\n}'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_info('2501.06243v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4",
      "metadata": {
        "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4"
      },
      "source": [
        "## Tool Schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c7d2260-452d-472a-b56e-326479cb18c9",
      "metadata": {
        "id": "7c7d2260-452d-472a-b56e-326479cb18c9"
      },
      "source": [
        "Esquema de cada herramienta que proporcionaremos al LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7",
      "metadata": {
        "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7"
      },
      "outputs": [],
      "source": [
        "# Esquema de herramientas adaptado para OpenAI/Nebius API (formato diferente a Anthropic)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_papers\",\n",
        "            \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The topic to search for\"\n",
        "                    },\n",
        "                    \"max_results\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Maximum number of results to retrieve\",\n",
        "                        \"default\": 5\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"topic\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"extract_info\",\n",
        "            \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"paper_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The ID of the paper to look for\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"paper_id\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2",
      "metadata": {
        "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2"
      },
      "source": [
        "## Tool Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4",
      "metadata": {
        "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4"
      },
      "source": [
        "Este código maneja el mapeo y la ejecución de herramientas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90790c0-efc4-4068-9c00-d2592d80bc30",
      "metadata": {
        "id": "c90790c0-efc4-4068-9c00-d2592d80bc30"
      },
      "outputs": [],
      "source": [
        "mapping_tool_function = {\n",
        "    \"search_papers\": search_papers,\n",
        "    \"extract_info\": extract_info\n",
        "}\n",
        "\n",
        "def execute_tool(tool_name, tool_args):\n",
        "\n",
        "    result = mapping_tool_function[tool_name](**tool_args)\n",
        "\n",
        "    if result is None:\n",
        "        result = \"The operation completed but didn't return any results.\"\n",
        "\n",
        "    elif isinstance(result, list):\n",
        "        result = ', '.join(result)\n",
        "\n",
        "    elif isinstance(result, dict):\n",
        "        # Convert dictionaries to formatted JSON strings\n",
        "        result = json.dumps(result, indent=2)\n",
        "\n",
        "    else:\n",
        "        # For any other type, convert using str()\n",
        "        result = str(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31",
      "metadata": {
        "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31"
      },
      "source": [
        "## Chatbot Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087",
      "metadata": {
        "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087"
      },
      "source": [
        "El chatbot responde a las consultas del usuario una por una, pero no mantiene memoria entre las consultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe662400-8506-464e-a3da-75a3d8848bac",
      "metadata": {
        "id": "fe662400-8506-464e-a3da-75a3d8848bac"
      },
      "outputs": [],
      "source": [
        "# Configurar API key de Nebius\n",
        "# load_dotenv()\n",
        "# from google.colab import userdata\n",
        "\n",
        "# API key de Nebius (JWT token)\n",
        "NEBIUS_API_KEY = \"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzcwNzQ1MDE2NTIyODIxMDgzNSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkwODk3ODIyNywidXVpZCI6IjE1ZGMxYTcyLTkwMzMtNDU1MS1hNTBiLWI0MDM1ODVlZmYyZiIsIm5hbWUiOiJOZWJpdXNLZXkiLCJleHBpcmVzX2F0IjoiMjAzMC0wNi0yOVQxNTo0Mzo0NyswMDAwIn0.6QhTkStPAH9_Dae2sbF1oU6XlVHbeY4kOb7e1icluwE\"\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = NEBIUS_API_KEY\n",
        "\n",
        "# Para Colab, también puedes usar:\n",
        "# os.environ['NEBIUS_API_KEY'] = userdata.get('NEBIUS_API_KEY')\n",
        "\n",
        "# Crear cliente de Nebius (compatible con OpenAI SDK)\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get('NEBIUS_API_KEY'),\n",
        "    base_url=\"https://api.nebius.ai/v1\"  # Ajusta según la URL real de Nebius\n",
        ")\n",
        "\n",
        "# Modelo multimodal de Nebius (ajusta según disponibilidad)\n",
        "NEBUS_MODEL = \"nebius-multimodal\"  # Cambia por el nombre real del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175586b4-acdf-4103-8039-134478a4f797",
      "metadata": {
        "id": "175586b4-acdf-4103-8039-134478a4f797"
      },
      "source": [
        "### Query Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a896e0-3f56-417e-aa51-c61756048593",
      "metadata": {
        "id": "12a896e0-3f56-417e-aa51-c61756048593"
      },
      "outputs": [],
      "source": [
        "def process_query(query):\n",
        "    \"\"\"\n",
        "    Procesa una consulta usando Nebius AI con function calling.\n",
        "    Adaptado de Anthropic API a OpenAI/Nebius API.\n",
        "    \"\"\"\n",
        "    messages = [{'role': 'user', 'content': query}]\n",
        "\n",
        "    # Primera llamada al modelo\n",
        "    response = client.chat.completions.create(\n",
        "        model=NEBUS_MODEL,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=2024\n",
        "    )\n",
        "\n",
        "    process_query = True\n",
        "    while process_query:\n",
        "        # Procesar la respuesta\n",
        "        message = response.choices[0].message\n",
        "        \n",
        "        # Construir el mensaje del asistente (formato OpenAI)\n",
        "        assistant_message = {'role': 'assistant'}\n",
        "        if message.content:\n",
        "            assistant_message['content'] = message.content\n",
        "        if message.tool_calls:\n",
        "            assistant_message['tool_calls'] = [\n",
        "                {\n",
        "                    'id': tc.id,\n",
        "                    'type': 'function',\n",
        "                    'function': {\n",
        "                        'name': tc.function.name,\n",
        "                        'arguments': tc.function.arguments\n",
        "                    }\n",
        "                }\n",
        "                for tc in message.tool_calls\n",
        "            ]\n",
        "        \n",
        "        messages.append(assistant_message)\n",
        "\n",
        "        # Si hay texto, imprimirlo\n",
        "        if message.content:\n",
        "            print(message.content)\n",
        "            # Si no hay tool_calls, terminamos\n",
        "            if not message.tool_calls:\n",
        "                process_query = False\n",
        "                break\n",
        "\n",
        "        # Si hay tool_calls, ejecutarlos\n",
        "        if message.tool_calls:\n",
        "            for tool_call in message.tool_calls:\n",
        "                tool_id = tool_call.id\n",
        "                tool_name = tool_call.function.name\n",
        "                tool_args = json.loads(tool_call.function.arguments)\n",
        "                \n",
        "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
        "                \n",
        "                # Ejecutar la herramienta\n",
        "                result = execute_tool(tool_name, tool_args)\n",
        "                \n",
        "                # Añadir el resultado de la herramienta a los mensajes (formato OpenAI)\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tool_id,\n",
        "                    \"content\": result\n",
        "                })\n",
        "            \n",
        "            # Hacer otra llamada con los resultados de las herramientas\n",
        "            response = client.chat.completions.create(\n",
        "                model=NEBUS_MODEL,\n",
        "                messages=messages,\n",
        "                tools=tools,\n",
        "                tool_choice=\"auto\",\n",
        "                max_tokens=2024\n",
        "            )\n",
        "            \n",
        "            # Si la respuesta final es solo texto, terminamos\n",
        "            if response.choices[0].message.content and not response.choices[0].message.tool_calls:\n",
        "                print(response.choices[0].message.content)\n",
        "                process_query = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9",
      "metadata": {
        "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9"
      },
      "source": [
        "### Chat Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16979cdc-81e9-432b-ba7f-e810b52961e8",
      "metadata": {
        "id": "16979cdc-81e9-432b-ba7f-e810b52961e8"
      },
      "outputs": [],
      "source": [
        "def chat_loop():\n",
        "    print(\"Type your queries or 'quit' to exit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\nQuery: \").strip()\n",
        "            if query.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            process_query(query)\n",
        "            print(\"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3",
      "metadata": {
        "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3"
      },
      "source": [
        "Prueba a interactuar con el chatbot. Aquí tienes un ejemplo de consulta:\n",
        "- Search for 2 papers on \"LLM Jailbreaking\"\n",
        "(o en castellano \"Busca 2 artículos sobre \"LLM Jailbreaking\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
        "outputId": "441ff409-cf8b-487c-e72a-969050c9f242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type your queries or 'quit' to exit.\n",
            "\n",
            "Query: Busca 2 artículos sobre \"LLM Jailbreaking\"\n",
            "Puedo buscar artículos sobre \"LLM Jailbreaking\" para ti. Voy a utilizar la herramienta de búsqueda para encontrar 2 artículos sobre este tema en arXiv.\n",
            "Calling tool search_papers with args {'topic': 'LLM Jailbreaking', 'max_results': 2}\n",
            "Results are saved in: papers/llm_jailbreaking/papers_info.json\n",
            "Ahora obtendré información detallada sobre estos dos artículos:\n",
            "Calling tool extract_info with args {'paper_id': '2405.20015v2'}\n",
            "Calling tool extract_info with args {'paper_id': '2312.04127v2'}\n",
            "Aquí te presento los 2 artículos sobre \"LLM Jailbreaking\" que encontré:\n",
            "\n",
            "### Artículo 1\n",
            "- **Título**: \"Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak\"\n",
            "- **Autores**: Zhenxing Niu, Yuyao Sun, Haoxuan Ji, Zheng Lin, Haichang Gao, Xinbo Gao, Gang Hua, Rong Jin\n",
            "- **Fecha de publicación**: 30 de mayo de 2024\n",
            "- **Resumen**: Este artículo se centra en los ataques de jailbreak contra grandes modelos de lenguaje (LLMs), provocando que generen contenido objetable en respuesta a consultas dañinas. A diferencia de métodos anteriores que apuntan directamente a los LLMs, este enfoque comienza construyendo un modelo de lenguaje multimodal (MLLM) sobre el LLM objetivo. Posteriormente, realizan un jailbreak eficiente del MLLM para obtener una incrustación de jailbreak, que finalmente convierten en un sufijo textual para ejecutar el jailbreak del LLM objetivo. Este enfoque indirecto es más eficiente, ya que los MLLMs son más vulnerables que los LLMs puros. Los experimentos demuestran que supera a los métodos actuales tanto en eficiencia como en efectividad.\n",
            "\n",
            "### Artículo 2\n",
            "- **Título**: \"Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak\"\n",
            "- **Autores**: Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, Bing Qin\n",
            "- **Fecha de publicación**: 7 de diciembre de 2023\n",
            "- **Resumen**: Esta investigación presenta un nuevo método automático de jailbreak llamado RADIAL, que elude los mecanismos de seguridad amplificando el potencial de los LLMs para generar respuestas afirmativas. La idea central es el \"Análisis de la Tendencia de Respuesta Inherente\", que identifica instrucciones del mundo real que pueden inducir a los LLMs a generar respuestas afirmativas. La estrategia correspondiente implica empalmar estratégicamente estas instrucciones alrededor de instrucciones maliciosas. El método logró un excelente rendimiento de ataque en instrucciones maliciosas en inglés con cinco LLMs avanzados de código abierto, manteniendo un rendimiento robusto en ataques multilingües contra instrucciones maliciosas en chino.\n",
            "\n",
            "Ambos artículos están disponibles en formato PDF a través de los enlaces proporcionados en la información de cada uno.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02d207b-e07d-49ff-bb03-7954aa86c167",
      "metadata": {
        "id": "d02d207b-e07d-49ff-bb03-7954aa86c167"
      },
      "source": [
        "## Resources\n",
        "\n",
        "**Adaptado para Nebius AI:**\n",
        "- Este notebook usa OpenAI SDK compatible con Nebius AI\n",
        "- Para function calling con OpenAI/Nebius: [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
        "- Ajusta `base_url` y `NEBUS_MODEL` según la documentación oficial de Nebius"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
