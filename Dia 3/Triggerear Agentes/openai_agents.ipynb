{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adaptación del Notebook para Nebius AI\n",
        "\n",
        "Este notebook ha sido adaptado para usar **Nebius AI** con el modelo **Llama 3.3** en lugar de OpenAI.\n",
        "\n",
        "## Cambios principales:\n",
        "- ✅ API key de Nebius configurada\n",
        "- ✅ Base URL de Nebius configurada (`https://api.studio.nebius.com/v1`)\n",
        "- ✅ Modelo cambiado a `meta-llama/Llama-3.3-70B-Instruct`\n",
        "- ✅ Cliente de OpenAI personalizado configurado para Nebius\n",
        "- ✅ Uso de `OpenAIChatCompletionsModel` con cliente personalizado para asegurar que el SDK use Nebius\n",
        "- ✅ Todas las instancias de modelos GPT reemplazadas por el modelo de Nebius\n",
        "\n",
        "## Configuración:\n",
        "- La API key y base URL se configuran en las primeras celdas del notebook\n",
        "- Se crea un cliente `AsyncOpenAI` configurado para Nebius\n",
        "- El modelo `NEBUS_MODEL` se crea usando `OpenAIChatCompletionsModel` con el cliente personalizado\n",
        "- Esto asegura que el SDK `openai-agents` use Nebius en lugar de OpenAI\n",
        "\n",
        "## Solución de problemas:\n",
        "Si encuentras errores de autenticación o conexión:\n",
        "1. Verifica que la API key de Nebius sea válida\n",
        "2. Asegúrate de que el modelo `meta-llama/Llama-3.3-70B-Instruct` esté disponible en tu cuenta\n",
        "3. El error de tracing (401) es normal y no afecta la funcionalidad principal\n",
        "4. El modelo ahora usa un cliente personalizado, por lo que debería funcionar correctamente con Nebius\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE8uvqPUP393"
      },
      "source": [
        "\n",
        "## ¿Qué es OpenAI Agents SDK?\n",
        "\n",
        "OpenAI lanzó **Agents SDK** en marzo de 2025 como un nuevo framework *open-source* para agentes.\n",
        "\n",
        "* Actualización del framework experimental *Swarm*.\n",
        "* Similar a frameworks como LangChain, Llama-Index, LangGraph...\n",
        "\n",
        "**Nota:** Este notebook ha sido adaptado para usar **Nebius AI** con el modelo **Llama 3.3** en lugar de OpenAI.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oTDfxmFP8iT"
      },
      "source": [
        "\n",
        "## Características Clave\n",
        "\n",
        "* Agent Loop: llamadas a herramientas, envío de resultados al LLM ...\n",
        "* Python-first: características nativas de Python en lugar de introducir nuevas capas de abstracción.\n",
        "* Sessions: gestión automática del historial de la conversación (sin tener que hacerlo manualmente)\n",
        "* Multi-Agentes: Patrón orquestador y handoff (coordinación y delegación entre múltiples agentes).\n",
        "* Guardrails: Validación de entrada/salida\n",
        "* Function Tools: Funciones Python como herramientas\n",
        "* Trazabilidad: Visualización, depuración y monitorización de los pasos del agente.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WioCQfxpP_WQ"
      },
      "source": [
        "## Instalación y Primer Agente\n",
        "\n",
        "**1. Instalación:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2QpeikVQuuj",
        "outputId": "550e6c32-4347-415e-c957-8ed309c6bd7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z-jbNkJND5Pe"
      },
      "outputs": [],
      "source": [
        "from agents import set_tracing_export_api_key\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "from agents.models.openai_chatcompletions import OpenAIChatCompletionsModel\n",
        "\n",
        "import os\n",
        "# from google.colab import userdata  # Comentado para uso local\n",
        "\n",
        "# Configurar API key de Nebius\n",
        "NEBIUS_API_KEY = \"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzcwNzQ1MDE2NTIyODIxMDgzNSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkwODk3ODIyNywidXVpZCI6IjE1ZGMxYTcyLTkwMzMtNDU1MS1hNTBiLWI0MDM1ODVlZmYyZiIsIm5hbWUiOiJOZWJpdXNLZXkiLCJleHBpcmVzX2F0IjoiMjAzMC0wNi0yOVQxNTo0Mzo0NyswMDAwIn0.6QhTkStPAH9_Dae2sbF1oU6XlVHbeY4kOb7e1icluwE\"\n",
        "\n",
        "# Configurar variables de entorno para Nebius\n",
        "os.environ['OPENAI_API_KEY'] = NEBIUS_API_KEY\n",
        "os.environ['OPENAI_BASE_URL'] = 'https://api.studio.nebius.com/v1'  # Base URL de Nebius\n",
        "\n",
        "# Crear cliente de OpenAI configurado para Nebius (síncrono y asíncrono)\n",
        "nebius_client = OpenAI(\n",
        "    api_key=NEBIUS_API_KEY,\n",
        "    base_url='https://api.studio.nebius.com/v1'\n",
        ")\n",
        "\n",
        "nebius_async_client = AsyncOpenAI(\n",
        "    api_key=NEBIUS_API_KEY,\n",
        "    base_url='https://api.studio.nebius.com/v1'\n",
        ")\n",
        "\n",
        "# Modelo de Nebius (Llama 3.3)\n",
        "NEBUS_MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "# Crear modelo de Nebius usando OpenAIChatCompletionsModel con cliente personalizado\n",
        "# Esto asegura que el SDK use Nebius en lugar de OpenAI\n",
        "NEBUS_MODEL = OpenAIChatCompletionsModel(\n",
        "    model=NEBUS_MODEL_NAME,\n",
        "    openai_client=nebius_async_client\n",
        ")\n",
        "\n",
        "# Para Colab, también puedes usar:\n",
        "# from google.colab import userdata\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get('NEBIUS_API_KEY')\n",
        "# os.environ['OPENAI_BASE_URL'] = 'https://api.studio.nebius.com/v1'\n",
        "\n",
        "# Nota: set_tracing_export_api_key puede fallar con Nebius ya que el tracing es específico de OpenAI\n",
        "# Comentamos esta línea si causa problemas\n",
        "try:\n",
        "    set_tracing_export_api_key(NEBIUS_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Advertencia: No se pudo configurar el tracing (esto es normal con Nebius): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yqRCcyMQpt-"
      },
      "source": [
        "\n",
        "**2. Crear un Agente Básico:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HZ_xecpQQ2sL"
      },
      "outputs": [],
      "source": [
        "from agents import Agent\n",
        "\n",
        "# Crear agente usando modelo de Nebius (Llama 3.3)\n",
        "# NEBUS_MODEL ya está configurado con el cliente de Nebius en la celda anterior\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"Eres un asistente útil que responde a las preguntas del usuario\",\n",
        "    model=NEBUS_MODEL  # Usando modelo de Nebius con cliente personalizado\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMgl_quCQCsV"
      },
      "source": [
        "**3. Ejecutando un Agente:**\n",
        "\n",
        "El SDK proporciona métodos para ejecutar agentes:\n",
        "\n",
        "* Runner.run() - Ejecución asíncrona.\n",
        "* Runner.run_sync() - Ejecución síncrona.\n",
        "* Runner.run_streamed() - Ejecución asíncrona con respuestas en streaming.\n",
        "\n",
        "**Ejemplo Básico (Asíncrono):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWrf6nVNQ62D",
        "outputId": "58588123-b304-4f90-8b80-716a792c30b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[non-fatal] Tracing client error 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Your authentication token is not from a valid issuer.\",\n",
            "    \"type\": \"invalid_request_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": \"invalid_issuer\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Claro! Aquí te dejo una historia corta:\n",
            "\n",
            "Había una vez, en un pequeño pueblo rodeado de montañas, un anciano llamado Kaito. Kaito era un hombre sencillo, pero con un corazón lleno de sabiduría y compasión. Pasaba sus días cultivando un jardín lleno de hierbas medicinales y flores silvestres, que utilizaba para curar a los habitantes del pueblo.\n",
            "\n",
            "Un día, una joven llamada Akira llegó al pueblo en busca de refugio. Estaba huyendo de una gran ciudad, donde se sentía sola y desesperada. Kaito la recibió con una sonrisa cálida y la invitó a quedarse en su casa.\n",
            "\n",
            "Mientras Akira se recuperaba de su viaje, Kaito le enseñó el arte de curar con hierbas y la importancia de la conexión con la naturaleza. Akira se sintió atraída por la sabiduría y la calma de Kaito, y pronto se encontró ayudándolo en el jardín y aprendiendo de él.\n",
            "\n",
            "Con el tiempo, Akira descubrió que el jardín de Kaito tenía un poder especial. Las flores y las hierbas que crecían allí no solo curaban el cuerpo, sino que también calmaban el alma y traían paz a aquellos que las tocaban. Akira se dio cuenta de que Kaito no solo era un curandero, sino un guardián de la naturaleza y un sabio que había encontrado la clave para vivir en armonía con el mundo.\n",
            "\n",
            "La historia de Akira y Kaito se convirtió en una leyenda en el pueblo, y la gente venía de lejos para visitar el jardín mágico y aprender de la sabiduría del anciano. Y Akira, que había llegado como una refugiada, encontró un nuevo hogar y un propósito en la vida, gracias a la bondad y la sabiduría de Kaito.\n",
            "\n",
            "¿Te gustó la historia?\n"
          ]
        }
      ],
      "source": [
        "from agents import Runner\n",
        "\n",
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"cuéntame una historia corta\"\n",
        ")\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHgLFIDwQFeZ"
      },
      "source": [
        "**4. Streaming de Respuestas:**\n",
        "\n",
        "Particularmente útil para aplicaciones de cara al usuario, ya que proporciona feedback inmediato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb1lc9A6RABi",
        "outputId": "be120d2a-8f5b-4a04-8462-528a759ec41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Claro! Aquí te dejo una historia corta que espero te guste:\n",
            "\n",
            "**La Tienda Mágica de Relojes**\n",
            "\n",
            "En un pequeño pueblo rodeado de montañas, había una tienda de relojes que parecía haber sido olvidada por el tiempo. La tienda se llamaba \"El Tic-Tac\" y estaba regentada por un anciano llamado Jorge. Jorge era un relojero habil"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[non-fatal] Tracing client error 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Your authentication token is not from a valid issuer.\",\n",
            "    \"type\": \"invalid_request_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": \"invalid_issuer\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idoso que pasaba sus días arreglando y construyendo relojes de todos tipos, desde los más simples hasta los más complejos.\n",
            "\n",
            "Un día, una joven llamada Sofía se perdió en el pueblo y se encontró frente a la tienda de Jorge. La puerta estaba entreabierta, así que Sofía la empujó y entró. Dentro, encontró una colección increíble de relojes que parecían tener vida propia. Había relojes que sonaban melodías, relojes que mostraban imágenes en movimiento y relojes que parecían hablar.\n",
            "\n",
            "Jorge salió de la trastienda y se acercó a Sofía. \"Bienvenida a mi tienda\", dijo con una sonrisa. \"Aquí, el tiempo no existe. Solo hay el tic-tac de los relojes\".\n",
            "\n",
            "Sofía se quedó fascinada por la tienda y pasó horas explorándola. Jorge le mostró sus creaciones más increíbles, incluyendo un reloj que parecía ser una máquina del tiempo. Sofía se rió y le preguntó si realmente funcionaba. Jorge sonrió y le dijo que solo funcionaba para aquellos que creían en la magia del tiempo.\n",
            "\n",
            "Sofía se despidió de Jorge y salió de la tienda, pero antes de irse, se dio cuenta de que su reloj de pulsera había dejado de funcionar. Lo miró y vio que la manecilla de los segundos se había detenido en un número extraño: 11:11. De repente, el reloj comenzó a sonar una melodía suave y Sofía se sintió transportada a un lugar desconocido.\n",
            "\n",
            "Cuando abrió los ojos, se encontró de vuelta en la tienda, pero esta vez, estaba llena de personas que parecían haber sido transportadas desde diferentes épocas y lugares. Había un caballero medieval, una dama victoriana, un astronauta y muchos más. Todos parecían haber sido traídos por los relojes de Jorge.\n",
            "\n",
            "Sofía se dio cuenta de que la tienda de relojes era un lugar mágico donde el tiempo no existía y donde la imaginación era la única limitación. Y desde ese día, Sofía visitó la tienda cada vez que podía, para explorar los secretos del tic-tac y descubrir nuevos mundos y maravillas.\n",
            "\n",
            "¿Te gustó la historia?"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"cuéntame una historia corta\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "    isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J8_lR8FQICQ"
      },
      "source": [
        "**5. REPL básico:**\n",
        "\n",
        "El SDK proporciona `run_demo_loop` para realizar pruebas rápidas e interactivas del comportamiento de un agente directamente en tu terminal. Un agente NO recuerda el histórico de la conversación automáticamente, PERO run_demo_loop añade soporte automático de histórico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqRyxJ5HaXLj",
        "outputId": "8dc445a4-4620-4b22-dae6-42a089e8213d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from agents import Agent, run_demo_loop\n",
        "\n",
        "# Crear agente usando modelo de Nebius\n",
        "agent = Agent(\n",
        "    name=\"Assistant\", \n",
        "    instructions=\"You are a helpful assistant.\",\n",
        "    model=NEBUS_MODEL  # Usando modelo de Nebius con cliente personalizado\n",
        ")\n",
        "await run_demo_loop(agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jdDBYmTZu4y"
      },
      "source": [
        "**6. Function Tools:**\n",
        "\n",
        "Convierte funciones Python en herramientas que el agente puede usar.\n",
        "\n",
        "**Definición:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0HAVsr6RITw"
      },
      "outputs": [],
      "source": [
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplica `x` e `y` para dar una respuesta precisa.\"\"\"\n",
        "    return x*y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EudGO10LQLGm"
      },
      "source": [
        "**Claves para una buena herramienta:**\n",
        "\n",
        "* Nombre de función claro.\n",
        "* Nombres de parámetros descriptivos.\n",
        "* Anotaciones de tipo para entradas y salidas.\n",
        "* Docstring explicativo (se convierte en la descripción de la herramienta).\n",
        "\n",
        "**Uso:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0g_KRKWRMZv"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "# Crear agente con herramientas usando modelo de Nebius (Llama 3.3)\n",
        "agent_con_herramientas = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"Usa las herramientas provistas para responder a la pregunta del usuario\",\n",
        "    model=NEBUS_MODEL,  # Usando Llama 3.3 de Nebius\n",
        "    tools=[multiply] # Lista de herramientas disponibles\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEUDWIqzf2qD"
      },
      "outputs": [],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent_con_herramientas,\n",
        "    input=\"Cuánto es 1.23456 multiplicado por 4.5678\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1XQUa_gBh0",
        "outputId": "e71c6496-681d-4ae8-a1eb-6fa345121db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Assistant\", ...)\n",
            "- Final output (str):\n",
            "    1.23456 multiplicado por 4.5678 es aproximadamente 5.6392.\n",
            "- 3 new item(s)\n",
            "- 2 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whkVpY3EgW7k",
        "outputId": "b474fa59-9bf5-4895-b040-db718d364a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.23456 multiplicado por 4.5678 es aproximadamente 5.6392.\n"
          ]
        }
      ],
      "source": [
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxobFK-9yHTC"
      },
      "source": [
        "![Traza automática en OpenAI](https://ikasten.io/images/2025-traza.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_NLDkwnQNoD"
      },
      "source": [
        "## Guardrails (Barreras de Seguridad - Guardarraíles)\n",
        "\n",
        "Hay dos tipos principales:\n",
        "\n",
        "- **Input guardrails:** Validan o filtran la entrada del usuario antes de que llegue a tu agente.\n",
        "- **Output guardrails:** Verifican y pueden bloquear o modificar la salida del agente antes de devolverla al usuario.\n",
        "\n",
        "Los guardrails se implementan como funciones que pueden usar cualquier lógica, incluyendo llamadas a otros LLMs, para decidir si permiten o bloquean un mensaje.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAUHMfLFQQGH"
      },
      "source": [
        "### Ejemplo: Construyendo un Input Guardrail\n",
        "\n",
        "Supongamos que quieres evitar que tu agente responda preguntas sobre opiniones políticas. Puedes usar un agente dedicado como guardrail para detectar este tipo de consultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KucyDm5RQWX"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# Definimos la estructura de salida para el agente guardrail\n",
        "class Alerta(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# Agente que revisa si la consulta es sobre conflictos bélicos\n",
        "# Usando modelo de Nebius con cliente personalizado\n",
        "war_agent = Agent(\n",
        "    name=\"Chequeo conflictos bélicos\",\n",
        "    instructions=\"Verifica si el usuario está preguntando sobre conflictos bélicos\",\n",
        "    output_type=Alerta,\n",
        "    model=NEBUS_MODEL  # Usando modelo de Nebius con cliente personalizado\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-raqX3-tQSzC"
      },
      "source": [
        "Puedes ejecutar este agente directamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w46SMi07RVGR",
        "outputId": "732e6e7f-d936-4801-b270-342549ddf1af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_triggered=False reasoning='La pregunta es sobre una operación matemática, específicamente una multiplicación, y no tiene relación con conflictos bélicos.'\n"
          ]
        }
      ],
      "source": [
        "query = \"¿Qué opinas de la invasión de Ucrania por parte de Rusia?\"\n",
        "# query = \"cuánto es 123*456?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=war_agent, input=query)\n",
        "print(result.final_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giMHj91IQVGb"
      },
      "source": [
        "### Integrando Guardrails con tu Agente\n",
        "Para usar este chequeo como guardrail, envuélvelo en una función decorada con `@input_guardrail`.\n",
        "\n",
        "<img src=\"https://ikasten.io/images/guardrail.png\" width=\"85%\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSUjmPvhRZfg"
      },
      "outputs": [],
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGhOxkPtb68A"
      },
      "source": [
        " Estas funciones deben aceptar un contexto (`ctx`), el agente y la entrada, y devolver un objeto `GuardrailFunctionOutput`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sop3qEhVGsfW"
      },
      "outputs": [],
      "source": [
        "@input_guardrail\n",
        "async def war_guardrail(\n",
        "    ctx: RunContextWrapper[None],\n",
        "    agent: Agent,\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # Usamos el war_agent para revisar la entrada\n",
        "    response = await Runner.run(starting_agent=war_agent, input=input, context=ctx.context)\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNdqLtMQZBM"
      },
      "source": [
        "Ahora, añade este guardrail a tu agente principal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRvSzf9HRc3X"
      },
      "outputs": [],
      "source": [
        "# Crear agente con guardrails usando modelo de Nebius (Llama 3.3)\n",
        "agent = Agent(\n",
        "    name=\"Asistente\",\n",
        "    instructions=(\n",
        "        \"Eres un asistente útil. Utiliza siempre las \"\n",
        "        \"herramientas proporcionadas cuando sea posible \"\n",
        "        \" y no dependas sólo de tu propio conocimiento.\"\n",
        "    ),\n",
        "    model=NEBUS_MODEL,  # Usando Llama 3.3 de Nebius\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[war_guardrail],  # Lista de guardrails de entrada\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPCV6h4Qbfx"
      },
      "source": [
        "Cuando ejecutes tu agente, el guardrail revisará cada entrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29WBmDd6Rhe-",
        "outputId": "d68153b9-3cd8-45be-da74-b4c6bdd9c00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2345 multiplicado por 6.892 es igual a 8.508174.\n"
          ]
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"¿Cuánto es 1.2345 * 6.892?\"\n",
        ")\n",
        "print(result.final_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyuKNmWWxaYO",
        "outputId": "5bbe6872-33d8-4ee7-c322-62496f63a23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Guardrail InputGuardrail triggered tripwire\n"
          ]
        }
      ],
      "source": [
        "# Ahora, probamos con una pregunta referida a un conflicto:\n",
        "try:\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=\"¿Qué opinas del asalto de Israel a la franja de Gaza?\"\n",
        "  )\n",
        "  print(result.final_output)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n",
        "# Lanza InputGuardrailTripwireTriggered: Guardrail InputGuardrail triggered tripwire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq9dm2CuRjUZ"
      },
      "source": [
        "Cuando el guardrail se activa, el SDK lanza una excepción `InputGuardrailTripwireTriggered`, evitando que el agente responda a consultas no permitidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zPAwHdxQdmb"
      },
      "source": [
        "\n",
        "### Output Guardrails\n",
        "\n",
        "Los guardrails de salida funcionan de manera muy similar, pero usan el decorador `@output_guardrail` y se configuran mediante el parámetro `output_guardrails` en tu agente. Esto permite aplicar políticas sobre lo que el agente puede decir, incluso después de procesar la entrada.\n",
        "\n",
        "Ejemplo:\n",
        "https://gist.github.com/juananpe/a0b8fea57630179fdddaa3f59139019c\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmAD_qtrQfK8"
      },
      "source": [
        "\n",
        "**Buenas prácticas:**\n",
        "- Utiliza guardrails para hacer cumplir políticas de negocio, legales o de seguridad.\n",
        "- Los guardrails pueden llamar a otros LLMs, pero recuerda que esto incrementa el coste y la latencia.\n",
        "- Puedes encadenar varios guardrails para una protección por capas.\n",
        "- Usa tanto input como output guardrails para una cobertura completa.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3VVbOCQQgzp"
      },
      "source": [
        "# 7. Agentes Conversacionales:\n",
        "\n",
        "El SDK facilita el mantenimiento del contexto en conversaciones multi-turno, bien gestionándolo de forma manual o a través de sesiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTik8ziuoISu"
      },
      "source": [
        "### 7.1 Mantenimiento manual del histórico de la conversación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7w99MUrRpBA"
      },
      "outputs": [],
      "source": [
        "\n",
        "result1 = await Runner.run(starting_agent=agent, input=\"recuerda el número 5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYExzl2CK9b",
        "outputId": "05293af9-32ef-4919-8891-e0558e4e9475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Asistente\", ...)\n",
            "- Final output (str):\n",
            "    He anotado el número 5. ¿Hay algo más en lo que te pueda ayudar?\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 1 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ],
      "source": [
        "print(result1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1ONy8RFn8aK"
      },
      "outputs": [],
      "source": [
        "result = await Runner.run(starting_agent=agent, input=\"cuál es el número que te dije que recordaras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_kBHzSKoDCW",
        "outputId": "b3fb82ad-e7d4-4e4e-d824-efb68ede2c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Asistente\", ...)\n",
            "- Final output (str):\n",
            "    Lo siento, pero no tengo la capacidad para recordar información de conversaciones anteriores. Si me das el número de nuevo, estaré encantado de ayudarte con lo que necesites.\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 1 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtVupsson5vB",
        "outputId": "d8cce3bd-217b-4da4-f515-4ab63624b140"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'recuerda el número 5', 'role': 'user'},\n",
              " {'id': 'msg_055212774878820e00692c757b657c81a3a47de8ea405d4095',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': 'He anotado el número 5. ¿Hay algo más en lo que te pueda ayudar?',\n",
              "    'type': 'output_text',\n",
              "    'logprobs': []}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result1.to_input_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5IveU1QRrCY"
      },
      "source": [
        "\n",
        "Para la siguiente interacción, pasar el historial:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K2FK3vURqpZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_history = result1.to_input_list()\n",
        "result2 = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=input_history + [{\"role\": \"user\", \"content\":\n",
        "    \"multiplica por 3 el último número que te pasé\"}]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcRy3536o1bJ",
        "outputId": "4433f2d1-f7f9-4462-dc6d-f942ef0332f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Asistente\", ...)\n",
            "- Final output (str):\n",
            "    La multiplicación de 5 por 3 es 15. ¿Necesitas algo más?\n",
            "- 3 new item(s)\n",
            "- 2 raw response(s)\n",
            "- 1 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ],
      "source": [
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ztK08NRuAj"
      },
      "source": [
        "El método `to_input_list()` convierte el resultado en un historial de mensajes formateado correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAShBnkZpcop"
      },
      "source": [
        "### 7.2 Mantenimiento automático del histórico de la conversación mediante SESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ8Va-wJqZH8",
        "outputId": "4e1ab1e3-7bc0-40f1-b83b-08c1a9c08b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nest-asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eau9-IiqjG-"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W1dgYOypcEZ",
        "outputId": "335ff3c2-296e-402f-c1a8-1ab3df82710a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "San Sebastián.\n",
            "Gipuzkoa.\n",
            "Aproximadamente 729.000 habitantes (2023).\n"
          ]
        }
      ],
      "source": [
        "from agents import Agent, Runner, SQLiteSession\n",
        "# from google.colab import userdata  # Comentado para uso local\n",
        "import os\n",
        "\n",
        "# Configurar API key de Nebius (ya debería estar configurada en celdas anteriores)\n",
        "# Si no está configurada, descomenta las siguientes líneas:\n",
        "# NEBIUS_API_KEY = \"tu_api_key_aqui\"\n",
        "# os.environ['OPENAI_API_KEY'] = NEBIUS_API_KEY\n",
        "# os.environ['OPENAI_BASE_URL'] = 'https://api.studio.nebius.com/v1'\n",
        "\n",
        "# Para Colab, también puedes usar:\n",
        "# from google.colab import userdata\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get('NEBIUS_API_KEY')\n",
        "# os.environ['OPENAI_BASE_URL'] = 'https://api.studio.nebius.com/v1'\n",
        "\n",
        "from agents import set_tracing_export_api_key\n",
        "set_tracing_export_api_key(os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create agent usando modelo de Nebius\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"Responde de forma muy concisa y breve.\",\n",
        "    model=NEBUS_MODEL  # Usando modelo de Nebius con cliente personalizado\n",
        ")\n",
        "\n",
        "# Create a session instance with a session ID\n",
        "session = SQLiteSession(\"conversation_123\", \"chat.db\")\n",
        "\n",
        "# First turn\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"¿En qué ciudad está El Peine de los Vientos?\",\n",
        "    session=session\n",
        ")\n",
        "print(result.final_output)  # \"Donostia\"\n",
        "\n",
        "# Second turn - agent automatically remembers previous context\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"¿A qué provincia pertenece?\",\n",
        "    session=session\n",
        ")\n",
        "print(result.final_output)  # \"Gipuzkoa\"\n",
        "\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"¿Cuántos habitantes tiene esa provincia?\",\n",
        "    session=session\n",
        ")\n",
        "print(result.final_output)  # \"Aproximadamente 730.000 habitantes.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mICn1UpqvTHz",
        "outputId": "70e78aa1-7e81-4595-d649-55f1d2d4d380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': '¿En qué ciudad está El Peine de los Vientos?', 'role': 'user'}, {'id': 'msg_02965af89df9dba700692c75d35a688190aa54c48183e47c86', 'content': [{'annotations': [], 'text': 'San Sebastián.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': '¿A qué provincia pertenece?', 'role': 'user'}, {'id': 'msg_02965af89df9dba700692c75d6529c8190949cabe4505e8dc7', 'content': [{'annotations': [], 'text': 'Gipuzkoa.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': '¿Cuántos habitantes tiene esa provincia?', 'role': 'user'}, {'id': 'msg_02965af89df9dba700692c75d8855481909617fcfd99e4d421', 'content': [{'annotations': [], 'text': 'Aproximadamente 729.000 habitantes (2023).', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}]\n",
            "6\n",
            "[{'annotations': [], 'text': 'Aproximadamente 729.000 habitantes (2023).', 'type': 'output_text', 'logprobs': []}]\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "# Create a session instance with a session ID\n",
        "session2 = SQLiteSession(\"conversation_123\", \"chat.db\")\n",
        "history = await session2.get_items()\n",
        "print(history)\n",
        "print(len(history))\n",
        "\n",
        "last_item = await session2.pop_item()\n",
        "print(last_item['content'])\n",
        "\n",
        "# get size of history\n",
        "print(len(await session2.get_items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OEf5gz8vbVj"
      },
      "source": [
        "Es posible continuar una sesión en cualquier momento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCZr8GR-sa4J",
        "outputId": "20dfe8a1-b836-4caa-acb6-0bd3ca1775d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "San Sebastián.\n"
          ]
        }
      ],
      "source": [
        "# Create agent usando modelo de Nebius\n",
        "agent2 = Agent(\n",
        "    name=\"Assistant2\",\n",
        "    instructions=\"Responde de forma muy concisa y breve.\",\n",
        "    model=NEBUS_MODEL  # Usando modelo de Nebius con cliente personalizado\n",
        ")\n",
        "\n",
        "# Create a session instance with a session ID\n",
        "session2 = SQLiteSession(\"conversation_123\", \"chat.db\")\n",
        "\n",
        "# First turn\n",
        "result2 = await Runner.run(\n",
        "    agent2,\n",
        "    \"¿de qué ciudad estamos hablando?\",\n",
        "    session=session2\n",
        ")\n",
        "print(result2.final_output)  # \"Donostia\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIimB9JBQikL"
      },
      "source": [
        "\n",
        "**Conclusión**\n",
        "\n",
        "OpenAI's Agents SDK (adaptado para Nebius AI) ofrece:\n",
        "\n",
        "* Un framework estructurado para construir agentes LLM con herramientas, guardrails y gestión de conversaciones.\n",
        "\n",
        "* Un enfoque Python-first que lo hace accesible y flexible.\n",
        "\n",
        "* Compatible con APIs compatibles con OpenAI (como Nebius AI), permitiendo usar modelos como Llama 3.3.\n",
        "\n",
        "**Nota:** Este notebook usa Nebius AI con el modelo Llama 3.3, demostrando la compatibilidad del SDK con proveedores alternativos.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfsGPszYQkUZ"
      },
      "source": [
        "**Ventajas clave:**\n",
        "\n",
        "* Streaming incorporado.\n",
        "\n",
        "* Guardrails estructurados.\n",
        "\n",
        "* Definición simple de herramientas.\n",
        "\n",
        "* Gestión de contexto conversacional.\n",
        "\n",
        "* Capacidades de trazado para depuración.\n",
        "\n",
        "Es una base sólida para construir aplicaciones de IA prácticas y robustas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
